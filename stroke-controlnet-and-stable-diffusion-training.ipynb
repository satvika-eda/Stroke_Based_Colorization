{"cells":[{"cell_type":"markdown","metadata":{},"source":["Satvika Eda, Divya Sri Bandaru & Dhriti Anjaria\n","13th April 2025"]},{"cell_type":"markdown","metadata":{},"source":["# Stroke-Based Image Colorization with Stable Diffusion & ControlNet\n","\n","This notebook contains the **fine-tuning of the pretrained ControlNet model** and **zero-shot inference using pretrained Stable Diffusion + fine tuned ControlNet** for stroke-based image colorization.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:28:35.114730Z","iopub.status.busy":"2025-04-23T22:28:35.114132Z","iopub.status.idle":"2025-04-23T22:28:38.765840Z","shell.execute_reply":"2025-04-23T22:28:38.765078Z","shell.execute_reply.started":"2025-04-23T22:28:35.114690Z"},"trusted":true},"outputs":[],"source":["import torch.multiprocessing as mp\n","mp.set_start_method('spawn')  # or 'forkserver'"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:28:38.767604Z","iopub.status.busy":"2025-04-23T22:28:38.767293Z","iopub.status.idle":"2025-04-23T22:28:41.795944Z","shell.execute_reply":"2025-04-23T22:28:41.795341Z","shell.execute_reply.started":"2025-04-23T22:28:38.767588Z"},"trusted":true},"outputs":[],"source":["import os\n","import glob\n","import random\n","import cv2\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as T\n","from torchvision import models\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["### Loading the Multi Cue Stroke Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:28:41.796963Z","iopub.status.busy":"2025-04-23T22:28:41.796615Z","iopub.status.idle":"2025-04-23T22:28:41.816497Z","shell.execute_reply":"2025-04-23T22:28:41.815994Z","shell.execute_reply.started":"2025-04-23T22:28:41.796940Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset class loaded successfully ✅\n"]}],"source":["import sys\n","# sys.path.remove('/kaggle/input/lab-seg-dataset')\n","sys.path.append(\"/kaggle/input/dataset-seg\")\n","\n","from dataset import MultiCueStrokeDataset\n","print(\"Dataset class loaded successfully ✅\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:28:41.817489Z","iopub.status.busy":"2025-04-23T22:28:41.817203Z","iopub.status.idle":"2025-04-23T22:28:41.881245Z","shell.execute_reply":"2025-04-23T22:28:41.880613Z","shell.execute_reply.started":"2025-04-23T22:28:41.817473Z"},"trusted":true},"outputs":[],"source":["# configuration parameters for training \n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","LR = 1e-5\n","ACCUM_STEPS = 2\n","IMG_SIZE = 256\n","BATCH_SIZE = 4\n","EPOCHS = 4"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:28:41.883666Z","iopub.status.busy":"2025-04-23T22:28:41.883402Z","iopub.status.idle":"2025-04-23T22:28:41.898181Z","shell.execute_reply":"2025-04-23T22:28:41.897572Z","shell.execute_reply.started":"2025-04-23T22:28:41.883623Z"},"trusted":true},"outputs":[],"source":["# === DataLoader wrapper with prefetching ===\n","def get_dataloader(image_paths, batch_size=4, num_workers=4, shuffle=True, img_size=IMG_SIZE, device='cuda'):\n","    dataset = MultiCueStrokeDataset(image_paths, img_size=img_size, device=device)\n","    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True, prefetch_factor=2)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:28:41.898998Z","iopub.status.busy":"2025-04-23T22:28:41.898731Z","iopub.status.idle":"2025-04-23T22:28:43.518140Z","shell.execute_reply":"2025-04-23T22:28:43.517592Z","shell.execute_reply.started":"2025-04-23T22:28:41.898977Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(64733,\n"," 50000,\n"," ['/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/train/n01440764/n01440764_3198.JPEG',\n","  '/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/train/n01440764/n01440764_10845.JPEG'],\n"," ['/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/val/ILSVRC2012_val_00003485.JPEG',\n","  '/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/val/ILSVRC2012_val_00021211.JPEG'])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import glob\n","\n","# all training image paths (.JPEG files) from the subset class folders\n","TRAIN_FOLDER = \"/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/train\"\n","VAL_FOLDER = \"/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/val\"\n","IMG_SIZE = IMG_SIZE\n","\n","subset_class_ids = [\n","    \"n01440764\", \"n01514859\", \"n01629819\", \"n01664065\", \"n01742172\",\n","    \"n01882714\", \"n01978455\", \"n02002556\", \"n02071294\", \"n02099601\",\n","    \"n02104029\", \"n02112137\", \"n02123045\", \"n02165456\", \"n02206856\",\n","    \"n02279972\", \"n02317335\", \"n02395406\", \"n02415577\", \"n02480495\",\n","    \"n02509815\", \"n02692877\", \"n02786058\", \"n02823428\", \"n02879718\",\n","    \"n02966193\", \"n03047690\", \"n03126707\", \"n03179701\", \"n03255030\",\n","    \"n03379051\", \"n03424325\", \"n03494278\", \"n03584829\", \"n03633091\",\n","    \"n03770439\", \"n03814639\", \"n03888257\", \"n03976657\", \"n04037443\",\n","    \"n04118538\", \"n04552348\", \"n02113799\", \"n02391049\", \"n03478589\",\n","    \"n03085013\", \"n03100240\", \"n03666591\", \"n03314780\", \"n02795169\"\n","]\n","\n","train_image_paths = []\n","for class_id in subset_class_ids:\n","    class_path = os.path.join(TRAIN_FOLDER, class_id)\n","    if os.path.exists(class_path):\n","        images = glob.glob(os.path.join(class_path, \"*.JPEG\"))\n","        train_image_paths.extend(images)\n","        \n","# all validation image paths (.JPEG files) from the val folder\n","val_image_paths = glob.glob(os.path.join(VAL_FOLDER, \"*.JPEG\"))\n","len(train_image_paths), len(val_image_paths), train_image_paths[:2], val_image_paths[:2]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:28:43.519138Z","iopub.status.busy":"2025-04-23T22:28:43.518912Z","iopub.status.idle":"2025-04-23T22:28:46.120318Z","shell.execute_reply":"2025-04-23T22:28:46.119718Z","shell.execute_reply.started":"2025-04-23T22:28:43.519120Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n","100%|██████████| 161M/161M [00:00<00:00, 213MB/s] \n"]}],"source":["# Training and validation Dataset\n","train_dataset = MultiCueStrokeDataset(train_image_paths, img_size=IMG_SIZE, device='cuda')\n","val_dataset = MultiCueStrokeDataset(val_image_paths, img_size=IMG_SIZE, device='cuda')"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:28:46.121297Z","iopub.status.busy":"2025-04-23T22:28:46.121109Z","iopub.status.idle":"2025-04-23T22:28:47.750087Z","shell.execute_reply":"2025-04-23T22:28:47.749503Z","shell.execute_reply.started":"2025-04-23T22:28:46.121282Z"},"trusted":true},"outputs":[],"source":["# Dataloader\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","train_loader = get_dataloader(train_image_paths, batch_size=BATCH_SIZE, device=device)\n","val_loader = get_dataloader(val_image_paths, batch_size=BATCH_SIZE, shuffle=False, device=device)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:28:47.751099Z","iopub.status.busy":"2025-04-23T22:28:47.750838Z","iopub.status.idle":"2025-04-23T22:29:30.266201Z","shell.execute_reply":"2025-04-23T22:29:30.265617Z","shell.execute_reply.started":"2025-04-23T22:28:47.751078Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-04-23 22:28:51.358629: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1745447331.560837      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1745447331.611078      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f990324bef2a4db0bb323f475b99aa0d","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb7dc6457fc4487f9813d86c1e0152b1","version_major":2,"version_minor":0},"text/plain":["diffusion_pytorch_model.safetensors:   0%|          | 0.00/1.45G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a7f89400fe994c48a23193513a8d084d","version_major":2,"version_minor":0},"text/plain":["model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0814030481154a83970a5faa2f052820","version_major":2,"version_minor":0},"text/plain":["Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0879d6b761854bcb939df38acec7edab","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6180f8a32b9d43d5986dd959d44ddbf8","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64801a8e9b12482daa61d57ee7927e5b","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/4.72k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5801967611654ad78ee498cbfbbfeebb","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4557fd0e023943c5bfdf4483043b3020","version_major":2,"version_minor":0},"text/plain":["scheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d43753e0b4ed4ca1b7100be40e294934","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1186574dc8644d7bdda501314389c18","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4bb1be8381a04844b39407b9fd9ea2bc","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d0150780dbe436ca8871b51a8298432","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8cf6f04db12448bb6cccd955a138715","version_major":2,"version_minor":0},"text/plain":["diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e39e20523b2f42c3bd8d8528f984aab1","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"15abee05d6534ce5a8f679cb704a2a49","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ed5faa600f1f4611ac5082eda9f2108e","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a5890f1097814dc7bfeaf84f5d9996d7","version_major":2,"version_minor":0},"text/plain":["diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf3625451fab4780b383fda2806384cf","version_major":2,"version_minor":0},"text/plain":["Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# LOADING PIPELINE\n","# ====================\n","# Loading the Stable Diffusion pipeline with ControlNet\n","# ControlNet model for Canny edge detection\n","# https://huggingface.co/lllyasviel/control_v11p_sd15_canny\n","# Stable Diffusion model\n","# https://huggingface.co/runwayml/stable-diffusion-v1-5\n","\n","from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n","\n","controlnet = ControlNetModel.from_pretrained(\n","    \"lllyasviel/control_v11p_sd15_canny\",\n","    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",")\n","\n","pipe = StableDiffusionControlNetPipeline.from_pretrained(\n","    \"runwayml/stable-diffusion-v1-5\",\n","    controlnet=controlnet,\n","    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",").to(DEVICE)\n","\n","pipe.vae.enable_tiling()\n","pipe.enable_attention_slicing()\n","# === FREEZE UNET ===\n","for param in pipe.unet.parameters():\n","    param.requires_grad = False"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:29:30.267456Z","iopub.status.busy":"2025-04-23T22:29:30.266960Z","iopub.status.idle":"2025-04-23T22:29:30.271107Z","shell.execute_reply":"2025-04-23T22:29:30.270417Z","shell.execute_reply.started":"2025-04-23T22:29:30.267433Z"},"trusted":true},"outputs":[],"source":["# LORA CONFIG\n","\n","# import torch\n","# import torch.nn as nn\n","# import torch.nn.functional as \n","\n","# class LoRALinear(nn.Module):\n","#     def __init__(self, original, r=4, alpha=16):\n","#         super().__init__()\n","#         self.original = original\n","#         self.lora_A = nn.Linear(original.in_features, r, bias=False)\n","#         self.lora_B = nn.Linear(r, original.out_features, bias=False)\n","#         self.scaling = alpha / r\n","\n","#         # Init weights\n","#         nn.init.kaiming_uniform_(self.lora_A.weight, a=5**0.5)\n","#         nn.init.zeros_(self.lora_B.weight)\n","\n","#     def forward(self, x):\n","#         # Force all components on same device\n","#         device = self.original.weight.device\n","#         x = x.to(device)\n","#         self.lora_A = self.lora_A.to(device)\n","#         self.lora_B = self.lora_B.to(device)\n","\n","#         lora_out = self.lora_B(self.lora_A(x)) * self.scaling\n","#         return self.original(x) + lora_out\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:29:30.271967Z","iopub.status.busy":"2025-04-23T22:29:30.271738Z","iopub.status.idle":"2025-04-23T22:29:39.638041Z","shell.execute_reply":"2025-04-23T22:29:39.637251Z","shell.execute_reply.started":"2025-04-23T22:29:30.271953Z"},"trusted":true},"outputs":[],"source":["# # === MANUALLY INJECT LORA INTO LINEAR PROJECTIONS (to_q, to_k, to_v) ===\n","# import torch.nn as nn\n","# def patch_controlnet_lora(controlnet, r=4, alpha=16):\n","#     for name, module in controlnet.named_modules():\n","#         if hasattr(module, \"to_q\") and isinstance(module.to_q, nn.Linear):\n","#             device = module.to_q.weight.device\n","#             module.to_q = LoRALinear(module.to_q, r=r, alpha=alpha).to(device)\n","#             module.to_k = LoRALinear(module.to_k, r=r, alpha=alpha).to(device)\n","#             module.to_v = LoRALinear(module.to_v, r=r, alpha=alpha).to(device)\n","#     return controlnet\n","\n","# pipe.controlnet = patch_controlnet_lora(pipe.controlnet)\n","\n","# # === COLLECT LORA PARAMS ONLY ===\n","# lora_params = [p for n, p in pipe.controlnet.named_parameters() if \"lora\" in n and p.requires_grad]\n","# optimizer = torch.optim.AdamW(lora_params, lr=LR)\n","# loss_fn = nn.MSELoss()\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:29:39.639148Z","iopub.status.busy":"2025-04-23T22:29:39.638887Z","iopub.status.idle":"2025-04-23T22:29:40.836353Z","shell.execute_reply":"2025-04-23T22:29:40.835569Z","shell.execute_reply.started":"2025-04-23T22:29:39.639123Z"},"trusted":true},"outputs":[],"source":["# converting lab tensor to rgb for control net training\n","\n","def lab_tensor_to_rgb(lab_tensor):\n","    lab_np = lab_tensor.detach().cpu().numpy()\n","    rgb_images = []\n","    for img in lab_np:\n","        L = (img[0] * 100).astype(np.float32)\n","        ab = (img[1:3] * 128).astype(np.float32)\n","        lab = np.stack([L, ab[0], ab[1]], axis=-1)\n","        lab = lab.astype(np.uint8)\n","        rgb = cv2.cvtColor(lab, cv2.COLOR_Lab2RGB)\n","        rgb_images.append(rgb)\n","    rgb_images = np.stack(rgb_images, axis=0)\n","    rgb_tensor = torch.from_numpy(rgb_images).permute(0, 3, 1, 2).float() / 255.0\n","    return rgb_tensor\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:29:40.837695Z","iopub.status.busy":"2025-04-23T22:29:40.837356Z","iopub.status.idle":"2025-04-23T22:29:41.645193Z","shell.execute_reply":"2025-04-23T22:29:41.644275Z","shell.execute_reply.started":"2025-04-23T22:29:40.837667Z"},"trusted":true},"outputs":[],"source":["# === TRAINING CONTROLNET ===\n","def train_controlnet(pipe, train_loader, val_loader, device=\"cuda\", save_dir=\"controlnet_ckpts\"):\n","    os.makedirs(save_dir, exist_ok=True)\n","    optimizer = torch.optim.AdamW(pipe.controlnet.parameters(), lr=1e-5)\n","    loss_fn = nn.MSELoss()\n","    ACCUM_STEPS = 1\n","    EPOCHS = 3\n","\n","    global_step = 0\n","\n","    for epoch in range(EPOCHS):\n","        pipe.controlnet.train()\n","        pipe.unet.eval()\n","        total_loss = 0\n","\n","        optimizer.zero_grad()\n","        for step, (inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch} - Training\")):\n","            inputs = inputs.to(device, dtype=pipe.vae.dtype)\n","            targets = targets.to(device)\n","\n","            # ✅ Convert AB strokes into RGB control image\n","            control_image = lab_tensor_to_rgb(inputs).to(device, dtype=pipe.vae.dtype)\n","\n","            # ✅ Dummy text prompt → encoder_hidden_states\n","            prompt = [\"a photo\"] * inputs.shape[0]\n","            with torch.no_grad():\n","                encoder_hidden_states = pipe.text_encoder(\n","                    pipe.tokenizer(prompt, padding=\"max_length\", max_length=77, return_tensors=\"pt\").input_ids.to(device)\n","                )[0]\n","\n","                latents = pipe.vae.encode(inputs).latent_dist.sample()\n","\n","            noise = torch.randn_like(latents)\n","            timesteps = torch.randint(0, 1000, (latents.shape[0],), device=device).long()\n","            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n","\n","            down_block_res_samples, mid_block_res_sample = pipe.controlnet(\n","                sample=noisy_latents,\n","                timestep=timesteps,\n","                encoder_hidden_states=encoder_hidden_states,\n","                controlnet_cond=control_image,\n","                return_dict=False\n","            )\n","\n","            pred = pipe.unet(\n","                noisy_latents,\n","                timesteps,\n","                encoder_hidden_states=encoder_hidden_states,\n","                down_block_additional_residuals=down_block_res_samples,\n","                mid_block_additional_residual=mid_block_res_sample,\n","            ).sample\n","\n","            loss = loss_fn(pred, noise)\n","            loss.backward()\n","\n","            if (step + 1) % ACCUM_STEPS == 0:\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","            total_loss += loss.item()\n","            global_step += 1\n","\n","        print(f\"Epoch {epoch} | Train Loss: {total_loss / len(train_loader):.4f}\")\n","        torch.save(pipe.controlnet.state_dict(), os.path.join(save_dir, f\"controlnet_epoch{epoch}.pth\"))\n","\n","        # === VALIDATION ===\n","        pipe.controlnet.eval()\n","        pipe.unet.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for inputs, targets in tqdm(val_loader, desc=f\"Epoch {epoch} - Validation\"):\n","                inputs = inputs.to(device, dtype=pipe.vae.dtype)\n","                targets = targets.to(device)\n","                control_image = lab_tensor_to_rgb(inputs).to(device, dtype=pipe.vae.dtype)\n","\n","                prompt = [\"a photo\"] * inputs.shape[0]\n","                encoder_hidden_states = pipe.text_encoder(\n","                    pipe.tokenizer(prompt, padding=\"max_length\", max_length=77, return_tensors=\"pt\").input_ids.to(device)\n","                )[0]\n","\n","                latents = pipe.vae.encode(inputs).latent_dist.sample()\n","                noise = torch.randn_like(latents)\n","                timesteps = torch.randint(0, 1000, (latents.shape[0],), device=device).long()\n","                noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n","\n","                down_block_res_samples, mid_block_res_sample = pipe.controlnet(\n","                    sample=noisy_latents,\n","                    timestep=timesteps,\n","                    encoder_hidden_states=encoder_hidden_states,\n","                    controlnet_cond=control_image,\n","                    return_dict=False\n","                )\n","\n","                pred = pipe.unet(\n","                    noisy_latents,\n","                    timesteps,\n","                    encoder_hidden_states=encoder_hidden_states,\n","                    down_block_additional_residuals=down_block_res_samples,\n","                    mid_block_additional_residual=mid_block_res_sample,\n","                ).sample\n","\n","                val_loss += loss_fn(pred, noise).item()\n","\n","            print(f\"Epoch {epoch} | Val Loss: {val_loss / len(val_loader):.4f}\")\n","            torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"execution_failed":"2025-04-24T02:47:49.761Z","iopub.execute_input":"2025-04-23T22:29:41.648317Z","iopub.status.busy":"2025-04-23T22:29:41.648006Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 0 - Training: 100%|██████████| 16184/16184 [3:27:16<00:00,  1.30it/s]  \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0 | Train Loss: nan\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 0 - Validation:   5%|▍         | 612/12500 [04:56<1:31:41,  2.16it/s]"]}],"source":["# TRAINING\n","train_controlnet(pipe, train_loader, val_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":4225553,"sourceId":6799,"sourceType":"competition"},{"datasetId":7234350,"sourceId":11534554,"sourceType":"datasetVersion"}],"dockerImageVersionId":31013,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":4}
