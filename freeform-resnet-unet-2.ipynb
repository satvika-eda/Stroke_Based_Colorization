{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6799,"databundleVersionId":4225553,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\n\n# List of specific ImageNet class IDs to include in the subset\nsubset_class_ids = [\n    \"n01440764\", \"n01491361\", \"n01498041\", \"n01514859\", \"n01530575\", \"n01616318\", \"n01622779\", \"n01629819\", \"n01641577\", \"n01664065\",\n    \"n01729322\", \"n01734418\", \"n01742172\", \"n01774750\", \"n01795545\", \"n01806143\", \"n01833805\", \"n01882714\", \"n01914609\", \"n01945685\",\n    \"n01978455\", \"n01985128\", \"n02002556\", \"n02006656\", \"n02011460\", \"n02058221\", \"n02071294\", \"n02085782\", \"n02088466\", \"n02099601\",\n    \"n02100583\", \"n02104029\", \"n02106550\", \"n02110063\", \"n02110958\", \"n02112137\", \"n02113023\", \"n02113799\", \"n02123045\", \"n02128757\",\n    \"n02132136\", \"n02165456\", \"n02190166\", \"n02206856\", \"n02226429\", \"n02233338\", \"n02256656\", \"n02279972\", \"n02317335\", \"n02346627\",\n    \"n02364673\", \"n02391049\", \"n02395406\", \"n02403003\", \"n02412080\", \"n02415577\", \"n02423022\", \"n02437312\", \"n02480495\", \"n02504458\",\n    \"n02509815\", \"n02640242\", \"n02692877\", \"n02708093\", \"n02786058\", \"n02795169\", \"n02814533\", \"n02823428\", \"n02837789\", \"n02879718\",\n    \"n02951358\", \"n02966193\", \"n03014705\", \"n03047690\", \"n03085013\", \"n03100240\", \"n03126707\", \"n03160309\", \"n03179701\", \"n03255030\",\n    \"n03272010\", \"n03314780\", \"n03379051\", \"n03417042\", \"n03424325\", \"n03444034\", \"n03478589\", \"n03494278\", \"n03584829\", \"n03633091\",\n    \"n03666591\", \"n03770439\", \"n03793489\", \"n03814639\", \"n03888257\", \"n03933933\", \"n03976657\", \"n04037443\", \"n04118538\", \"n04552348\"\n]\n\n# Directories\nimagenet_train_dir = \"/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/train\"\nsubset_dir = \"/kaggle/working/imagenet_subset/train\"\n\n# Create target root directory\nos.makedirs(subset_dir, exist_ok=True)\n\n# Iterate through specified class IDs\nfor class_id in subset_class_ids:\n    src_class_dir = os.path.join(imagenet_train_dir, class_id)\n    dst_class_dir = os.path.join(subset_dir, class_id)\n    os.makedirs(dst_class_dir, exist_ok=True)\n    \n    # List all images\n    images = sorted(os.listdir(src_class_dir))\n    \n    # Copy all images\n    for img_name in images:\n        src_img_path = os.path.join(src_class_dir, img_name)\n        dst_img_path = os.path.join(dst_class_dir, img_name)\n        shutil.copyfile(src_img_path, dst_img_path)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T18:29:37.105820Z","iopub.execute_input":"2025-04-23T18:29:37.106032Z","iopub.status.idle":"2025-04-23T18:55:02.371160Z","shell.execute_reply.started":"2025-04-23T18:29:37.106015Z","shell.execute_reply":"2025-04-23T18:55:02.370374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader# from torchvision import transforms\nfrom PIL import Image\nimport os\n\nclass ImageNetColorizationDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.image_paths = []\n        for subdir, _, files in os.walk(root_dir):\n            for file in files:\n                if file.endswith('.JPEG') or file.endswith('.jpg'):\n                    self.image_paths.append(os.path.join(subdir, file))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        try:\n            img = Image.open(img_path).convert(\"RGB\").resize((256, 256))\n            img = np.array(img)\n    \n            lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n            L = lab[:, :, 0] / 255.0\n            ab = lab[:, :, 1:] / 128.0\n    \n            L = torch.from_numpy(L).unsqueeze(0).float()        # [1, H, W]\n            ab = torch.from_numpy(ab.transpose((2, 0, 1))).float()  # [2, H, W]\n    \n            return L, ab\n        except Exception as e:\n            print(f\"Failed to load {img_path}: {e}\")\n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T18:55:31.001106Z","iopub.execute_input":"2025-04-23T18:55:31.001358Z","iopub.status.idle":"2025-04-23T18:55:35.140207Z","shell.execute_reply.started":"2025-04-23T18:55:31.001339Z","shell.execute_reply":"2025-04-23T18:55:35.139654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms\n\ntransform = transforms.ToTensor()\nimagenet_dataset = ImageNetColorizationDataset('/kaggle/working/imagenet_subset/train', transform=transform)\ntrain_loader = DataLoader(\n    imagenet_dataset,\n    batch_size=32,                  # Increase batch size if memory allows\n    shuffle=True,\n    num_workers=8,                  # Increase workers (rule: 4 * num_GPUs)\n    pin_memory=True,                # Keep this for GPU transfer efficiency\n    prefetch_factor=2,              # Prefetch batches\n    persistent_workers=True,        # Keep workers alive between epochs\n    drop_last=True                  # Slightly faster by dropping incomplete batches\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T18:55:35.141217Z","iopub.execute_input":"2025-04-23T18:55:35.141524Z","iopub.status.idle":"2025-04-23T18:55:39.901677Z","shell.execute_reply.started":"2025-04-23T18:55:35.141501Z","shell.execute_reply":"2025-04-23T18:55:39.900992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\ndef rgb_to_lab(img_rgb):\n    img_lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2LAB)\n    L, a, b = cv2.split(img_lab)\n    return L / 255.0, a / 128.0, b / 128.0  # normalize\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T18:55:43.347002Z","iopub.execute_input":"2025-04-23T18:55:43.347594Z","iopub.status.idle":"2025-04-23T18:55:43.662673Z","shell.execute_reply.started":"2025-04-23T18:55:43.347570Z","shell.execute_reply":"2025-04-23T18:55:43.662114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.nn.functional as F\n\nclass ResNetUNetColor(nn.Module):\n    def __init__(self):\n        super(ResNetUNetColor, self).__init__()\n\n        # Load pretrained ResNet18\n        resnet = models.resnet18(pretrained=True)\n\n        # Use only first conv block (accepts RGB), we’ll modify to accept 1-channel input\n        self.input_conv = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\n        # Extract ResNet layers for encoder\n        self.bn1 = resnet.bn1\n        self.relu = resnet.relu\n        self.maxpool = resnet.maxpool\n\n        self.layer1 = resnet.layer1  # [64, 64, 64]\n        self.layer2 = resnet.layer2  # [128, 32, 32]\n        self.layer3 = resnet.layer3  # [256, 16, 16]\n        self.layer4 = resnet.layer4  # [512, 8, 8]\n\n        # Decoder\n        self.up1 = self.up_block(512, 256)\n        self.up2 = self.up_block(512, 128)  # skip from layer3\n        self.up3 = self.up_block(256, 64)   # skip from layer2\n        self.up4 = self.up_block(128, 64)   # skip from layer1\n        self.up5 = nn.Sequential(           # final upsampling to 256×256\n            nn.ConvTranspose2d(64, 32, 2, stride=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 2, kernel_size=1)  # Output ab channels\n        )\n\n    def up_block(self, in_ch, out_ch):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        # Encoder\n        x1 = self.relu(self.bn1(self.input_conv(x)))  # [64, 128, 128]\n        x2 = self.layer1(self.maxpool(x1))            # [64, 64, 64]\n        x3 = self.layer2(x2)                          # [128, 32, 32]\n        x4 = self.layer3(x3)                          # [256, 16, 16]\n        x5 = self.layer4(x4)                          # [512, 8, 8]\n\n        # Decoder with skip connections\n        d1 = self.up1(x5)                             # [256, 16, 16]\n        d2 = self.up2(torch.cat([d1, x4], dim=1))     # [128, 32, 32]\n        d3 = self.up3(torch.cat([d2, x3], dim=1))     # [64, 64, 64]\n        d4 = self.up4(torch.cat([d3, x2], dim=1))     # [64, 128, 128]\n        out = self.up5(d4)                            # [2, 256, 256]\n\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T18:55:44.586810Z","iopub.execute_input":"2025-04-23T18:55:44.587090Z","iopub.status.idle":"2025-04-23T18:55:44.596206Z","shell.execute_reply.started":"2025-04-23T18:55:44.587068Z","shell.execute_reply":"2025-04-23T18:55:44.595570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = ResNetUNetColor().to(device)\ncriterion = nn.MSELoss()\n# 3. Now define the optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T18:56:14.834709Z","iopub.execute_input":"2025-04-23T18:56:14.835395Z","iopub.status.idle":"2025-04-23T18:56:15.334090Z","shell.execute_reply.started":"2025-04-23T18:56:14.835369Z","shell.execute_reply":"2025-04-23T18:56:15.333573Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n\n    for L, ab in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        L, ab = L.to(device), ab.to(device)\n\n        optimizer.zero_grad()\n        output_ab = model(L)\n        loss = criterion(output_ab, ab)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n\n    checkpoint_path = f\"/kaggle/working/resnet_colorization_model_epoch_{epoch+1}.pth\"\n    torch.save(model.state_dict(), checkpoint_path)\n    print(f\"Model saved to {checkpoint_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T18:56:34.846128Z","iopub.execute_input":"2025-04-23T18:56:34.846720Z","iopub.status.idle":"2025-04-23T20:24:44.870823Z","shell.execute_reply.started":"2025-04-23T18:56:34.846693Z","shell.execute_reply":"2025-04-23T20:24:44.869710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport torch\n\ndef visualize_colorization(model, dataloader, device):\n    model.eval()\n    with torch.no_grad():\n        for L, ab in dataloader:\n            L, ab = L.to(device), ab.to(device)\n            output_ab = model(L)\n\n            # Get first sample in batch\n            L_img = L[0].cpu().numpy()[0] * 255.0  # [256, 256]\n            ab_gt = ab[0].cpu().numpy().transpose(1, 2, 0) * 128.0  # [256, 256, 2]\n            ab_pred = output_ab[0].cpu().numpy().transpose(1, 2, 0) * 128.0\n\n            # Construct LAB images\n            lab_gt = np.zeros((256, 256, 3), dtype=np.float32)\n            lab_pred = np.zeros((256, 256, 3), dtype=np.float32)\n            lab_gt[:, :, 0] = L_img\n            lab_gt[:, :, 1:] = ab_gt\n            lab_pred[:, :, 0] = L_img\n            lab_pred[:, :, 1:] = ab_pred\n\n            # Convert LAB to RGB\n            rgb_gt = cv2.cvtColor(lab_gt.astype(np.uint8), cv2.COLOR_LAB2RGB)\n            rgb_pred = cv2.cvtColor(lab_pred.astype(np.uint8), cv2.COLOR_LAB2RGB)\n\n            # Grayscale visualization\n            gray_img = cv2.cvtColor(rgb_gt, cv2.COLOR_RGB2GRAY)\n\n            # Plot\n            plt.figure(figsize=(12, 4))\n            plt.subplot(1, 3, 1)\n            plt.imshow(rgb_gt)\n            plt.title(\"Original\")\n            plt.axis('off')\n\n            plt.subplot(1, 3, 2)\n            plt.imshow(gray_img, cmap='gray')\n            plt.title(\"Input Grayscale\")\n            plt.axis('off')\n\n            plt.subplot(1, 3, 3)\n            plt.imshow(rgb_pred)\n            plt.title(\"Reconstructed\")\n            plt.axis('off')\n\n            plt.tight_layout()\n            plt.show()\n            break  # Show only one batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T20:26:08.549200Z","iopub.execute_input":"2025-04-23T20:26:08.550163Z","iopub.status.idle":"2025-04-23T20:26:08.561044Z","shell.execute_reply.started":"2025-04-23T20:26:08.550125Z","shell.execute_reply":"2025-04-23T20:26:08.560314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvisualize_colorization(model, train_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T20:33:12.471201Z","iopub.execute_input":"2025-04-23T20:33:12.471840Z","iopub.status.idle":"2025-04-23T20:33:14.798117Z","shell.execute_reply.started":"2025-04-23T20:33:12.471819Z","shell.execute_reply":"2025-04-23T20:33:14.797295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport cv2\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\ndef colorize_single_image(model, image_path, device):\n    model.eval()\n\n    # Load and resize image\n    img = Image.open(image_path).convert(\"RGB\").resize((256, 256))\n    img_np = np.array(img)\n\n    # Convert to LAB and extract L channel\n    lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n    L = lab[:, :, 0] / 255.0  # Normalize L channel\n\n    # Convert to tensor\n    L_tensor = torch.from_numpy(L).unsqueeze(0).unsqueeze(0).float().to(device)  # [1, 1, 256, 256]\n\n    # Forward pass\n    with torch.no_grad():\n        ab_pred = model(L_tensor)[0].cpu().numpy().transpose(1, 2, 0) * 128.0\n\n    # Reconstruct LAB image and convert to RGB\n    L_img = L * 255.0\n    lab_pred = np.zeros((256, 256, 3), dtype=np.float32)\n    lab_pred[:, :, 0] = L_img\n    lab_pred[:, :, 1:] = ab_pred\n    rgb_pred = cv2.cvtColor(lab_pred.astype(np.uint8), cv2.COLOR_LAB2RGB)\n\n    # Show result\n    plt.figure(figsize=(8, 4))\n    plt.subplot(1, 2, 1)\n    plt.imshow(img_np)\n    plt.title(\"Original Image\")\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(rgb_pred)\n    plt.title(\"Colorized Output\")\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T20:34:47.937054Z","iopub.execute_input":"2025-04-23T20:34:47.937794Z","iopub.status.idle":"2025-04-23T20:34:47.944647Z","shell.execute_reply.started":"2025-04-23T20:34:47.937770Z","shell.execute_reply":"2025-04-23T20:34:47.943907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = \"/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/test/ILSVRC2012_test_00000004.JPEG\"  # Upload your own image to input folder\ncolorize_single_image(model, image_path, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T20:35:08.371649Z","iopub.execute_input":"2025-04-23T20:35:08.372323Z","iopub.status.idle":"2025-04-23T20:35:08.669256Z","shell.execute_reply.started":"2025-04-23T20:35:08.372298Z","shell.execute_reply":"2025-04-23T20:35:08.668632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}